{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab382210",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importa le librerie necessarie\n",
    "import os\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "import h5py\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Verifica disponibilità GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Dispositivo utilizzato: {device}\")\n",
    "print(f\"CUDA disponibile: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"Memoria GPU: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dac8f71a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Carica il modello CONCH 1.5\n",
    "from src.trident.patch_encoder_models.load import encoder_factory\n",
    "\n",
    "# Carica CONCH 1.5\n",
    "print(\"Caricamento modello CONCH 1.5...\")\n",
    "patch_encoder = encoder_factory('conch_v15')\n",
    "patch_encoder.to(device)\n",
    "patch_encoder.eval()\n",
    "\n",
    "print(f\"Modello caricato: {patch_encoder.enc_name}\")\n",
    "print(f\"Dimensione features: {patch_encoder.n_features}\")\n",
    "print(f\"Parametri del modello: {sum(p.numel() for p in patch_encoder.parameters()):,}\")\n",
    "print(\"Modello impostato in modalità eval ✓\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d040231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configurazione percorsi e parametri\n",
    "# Modifica questi percorsi secondo la tua configurazione\n",
    "HEST_PATCHES_DIR = \"/path/to/hest/patches\"  # Directory contenente i file H5 delle patch HEST\n",
    "OUTPUT_DIR = \"/path/to/output/features\"     # Directory dove salvare le features estratte\n",
    "BATCH_SIZE = 32                             # Batch size per l'inferenza\n",
    "\n",
    "# Crea la directory di output se non esiste\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"Directory di output: {OUTPUT_DIR}\")\n",
    "\n",
    "# Trova tutti i file H5 delle patch\n",
    "def find_hest_patch_files(patches_dir, pattern='patches.h5'):\n",
    "    \"\"\"Trova tutti i file H5 delle patch HEST.\"\"\"\n",
    "    patch_files = []\n",
    "    for root, dirs, files in os.walk(patches_dir):\n",
    "        for file in files:\n",
    "            if file.endswith(pattern):\n",
    "                patch_files.append(os.path.join(root, file))\n",
    "    return patch_files\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15fe5eee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzioni per caricare e processare le patch HEST\n",
    "def load_patches_from_h5(h5_path):\n",
    "    \"\"\"Carica patch e metadati da file H5 HEST.\"\"\"\n",
    "    data = {}\n",
    "    print(f\"Caricamento patch da: {h5_path}\")\n",
    "    \n",
    "    with h5py.File(h5_path, 'r') as f:\n",
    "        # Stampa le chiavi disponibili nel file\n",
    "        print(f\"Chiavi disponibili nel file H5: {list(f.keys())}\")\n",
    "        \n",
    "        # Carica patches (immagini)\n",
    "        if 'imgs' in f:\n",
    "            data['patches'] = f['imgs'][:]\n",
    "            print(f\"Caricate {len(data['patches'])} patch dalla chiave 'imgs'\")\n",
    "        elif 'img' in f:\n",
    "            data['patches'] = f['img'][:]\n",
    "            print(f\"Caricate {len(data['patches'])} patch dalla chiave 'img'\")\n",
    "        else:\n",
    "            raise KeyError(\"Nessun dato immagine trovato nel file H5. Attese chiavi 'imgs' o 'img'.\")\n",
    "        \n",
    "        # Carica coordinate se disponibili\n",
    "        if 'coords' in f:\n",
    "            data['coords'] = f['coords'][:]\n",
    "            print(f\"Caricate coordinate: {data['coords'].shape}\")\n",
    "        \n",
    "        # Carica barcodes se disponibili\n",
    "        if 'barcode' in f:\n",
    "            data['barcodes'] = f['barcode'][:]\n",
    "            print(f\"Caricati {len(data['barcodes'])} barcodes\")\n",
    "        \n",
    "        # Carica altri metadati\n",
    "        for key in f.keys():\n",
    "            if key not in ['imgs', 'img', 'coords', 'barcode']:\n",
    "                try:\n",
    "                    data[key] = f[key][:]\n",
    "                    print(f\"Caricati metadati '{key}': {data[key].shape}\")\n",
    "                except:\n",
    "                    print(f\"Impossibile caricare metadati '{key}'\")\n",
    "    \n",
    "    print(f\"Forma delle patch: {data['patches'].shape}\")\n",
    "    print(f\"Tipo di dati: {data['patches'].dtype}\")\n",
    "    return data\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "814dc500",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funzione per l'inferenza con CONCH 1.5\n",
    "def extract_patch_features(patches, patch_encoder, device, batch_size=32):\n",
    "    \"\"\"Estrae features dalle patch usando CONCH 1.5.\"\"\"\n",
    "    patch_encoder.eval()\n",
    "    features = []\n",
    "    \n",
    "    print(f\"Estrazione features da {len(patches)} patch...\")\n",
    "    print(f\"Batch size: {batch_size}\")\n",
    "    \n",
    "    # Preprocessing: normalizzazione\n",
    "    if patches.dtype == np.uint8:\n",
    "        patches = patches.astype(np.float32) / 255.0\n",
    "    \n",
    "    # Converte in tensor PyTorch e cambia dimensioni (N, H, W, C) -> (N, C, H, W)\n",
    "    patches_tensor = torch.from_numpy(patches).permute(0, 3, 1, 2)\n",
    "    print(f\"Forma tensor patches: {patches_tensor.shape}\")\n",
    "    \n",
    "    # Processamento in batch\n",
    "    with torch.no_grad():\n",
    "        for i in tqdm(range(0, len(patches_tensor), batch_size), desc=\"Estrazione features\"):\n",
    "            batch = patches_tensor[i:i+batch_size].to(device)\n",
    "            \n",
    "            # Inferenza\n",
    "            batch_features = patch_encoder(batch)\n",
    "            features.append(batch_features.cpu().numpy())\n",
    "    \n",
    "    # Concatena tutti i batch\n",
    "    features = np.concatenate(features, axis=0)\n",
    "    print(f\"Features estratte: {features.shape}\")\n",
    "    print(f\"Dimensionalità per patch: {features.shape[1]}\")\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d49c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Salva le features estratte in una directory\n",
    "def save_features_to_directory(features, metadata, output_dir, sample_name=\"example_sample\"):\n",
    "    \"\"\"Salva features e metadati in una directory organizzata.\"\"\"\n",
    "    \n",
    "    # Crea directory per il campione\n",
    "    sample_dir = os.path.join(output_dir, sample_name)\n",
    "    os.makedirs(sample_dir, exist_ok=True)\n",
    "    \n",
    "    # Salva features in formato H5\n",
    "    features_path = os.path.join(sample_dir, f\"{sample_name}_features.h5\")\n",
    "    \n",
    "    with h5py.File(features_path, 'w') as f:\n",
    "        # Salva features\n",
    "        f.create_dataset('features', data=features)\n",
    "        \n",
    "        # Salva metadati\n",
    "        if 'coords' in metadata:\n",
    "            f.create_dataset('coords', data=metadata['coords'])\n",
    "            \n",
    "        if 'barcodes' in metadata:\n",
    "            # Converte strings in bytes per H5\n",
    "            barcodes_bytes = [b.encode('utf-8') if isinstance(b, str) else b for b in metadata['barcodes']]\n",
    "            f.create_dataset('barcodes', data=barcodes_bytes)\n",
    "        \n",
    "        # Salva informazioni sul modello\n",
    "        f.attrs['model_name'] = 'conch_v15'\n",
    "        f.attrs['feature_dim'] = features.shape[1]\n",
    "        f.attrs['n_patches'] = features.shape[0]\n",
    "        f.attrs['processing_date'] = str(np.datetime64('now'))\n",
    "    \n",
    "    print(f\"Features salvate in: {features_path}\")\n",
    "    \n",
    "    # Salva anche in formato NumPy per facilità di accesso\n",
    "    numpy_path = os.path.join(sample_dir, f\"{sample_name}_features.npy\")\n",
    "    np.save(numpy_path, features)\n",
    "    print(f\"Features salvate anche in: {numpy_path}\")\n",
    "    \n",
    "    # Salva metadati in formato JSON\n",
    "    import json\n",
    "    metadata_path = os.path.join(sample_dir, f\"{sample_name}_metadata.json\")\n",
    "    \n",
    "    # Prepara metadati per JSON\n",
    "    json_metadata = {\n",
    "        'model_name': 'conch_v15',\n",
    "        'feature_dim': int(features.shape[1]),\n",
    "        'n_patches': int(features.shape[0]),\n",
    "        'processing_date': str(np.datetime64('now')),\n",
    "        'coords_shape': metadata['coords'].shape if 'coords' in metadata else None,\n",
    "        'n_barcodes': len(metadata['barcodes']) if 'barcodes' in metadata else None\n",
    "    }\n",
    "    \n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(json_metadata, f, indent=2)\n",
    "    \n",
    "    print(f\"Metadati salvati in: {metadata_path}\")\n",
    "    return sample_dir\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
