{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3f519a87",
   "metadata": {},
   "source": [
    "# WSI Feature Extraction Pipeline\n",
    "\n",
    "This notebook demonstrates how to extract and save patch features from Whole Slide Images (WSIs) using a custom pipeline manager that tracks processing status and only performs missing operations.\n",
    "\n",
    "## Key Features:\n",
    "- **Smart State Management**: Tracks which slides have completed segmentation and feature extraction\n",
    "- **Resume Capability**: Only processes missing operations, avoiding redundant computation\n",
    "- **Organized Output Structure**: Logical folder hierarchy for different processing stages\n",
    "- **Professional Implementation**: Clean, maintainable code with proper error handling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2cb86b",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries and Custom Module\n",
    "\n",
    "First, we'll import the necessary libraries and our custom pipeline manager module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "506e76c4",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/vcivale/WSI-RL-Tiles-Selection_3/.venv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "GPU: NVIDIA GeForce RTX 2080 Ti\n",
      "GPU Memory: 10.8 GB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Import our custom pipeline manager\n",
    "from trident_pipeline_manager import PipelineManager, PipelineConfig\n",
    "\n",
    "# Set up device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Display GPU information if available\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c70137",
   "metadata": {},
   "source": [
    "## 2. Initialize Manager and Set Up Paths\n",
    "\n",
    "Configure the pipeline parameters and initialize the manager for the HF dataset structure. This dataset already contains pre-extracted patches and tissue segmentation masks, so we'll adapt the pipeline accordingly.\n",
    "\n",
    "### HF Dataset Structure:\n",
    "- **wsis/**: H&E stained Whole Slide Images in pyramidal Generic TIFF format\n",
    "- **patches/**: Pre-extracted 256x256 H&E patches (0.5µm/px) in .h5 format\n",
    "- **tissue_seg/**: Pre-computed tissue segmentation masks and contours\n",
    "- **st/**: Spatial transcriptomics expressions in scanpy .h5ad format\n",
    "- **metadata/**: Sample metadata files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092993d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration:\n",
      "  WSI Directory: /home/vcivale/WSI-RL-Tiles-Selection_3/data/interim\n",
      "  Output Directory: /home/vcivale/WSI-RL-Tiles-Selection_3/data/processed\n",
      "  Patch Encoder: conch_v15\n",
      "  Magnification: 20x\n",
      "  Patch Size: 512px\n",
      "  Batch Size: 32\n",
      "  WSI Reader: cucim\n",
      "  MPP: 0.25\n"
     ]
    }
   ],
   "source": [
    "# Configuration - Update these paths according to your HF dataset setup\n",
    "BASE_DATA_DIR = \"/home/vcivale/WSI-RL-Tiles-Selection_3/data\"  # Base directory for HF dataset\n",
    "WSI_DIR = os.path.join(BASE_DATA_DIR, \"wsis\")  # Directory containing WSI files from HF dataset\n",
    "OUTPUT_DIR = \"/home/vcivale/WSI-RL-Tiles-Selection_3/data/processed\"  # Directory to save processed outputs\n",
    "WSI_EXTENSIONS = ['.tiff', '.tif', '.svs', '.ndpi', '.mrxs']  # Supported WSI formats\n",
    "\n",
    "# HF Dataset specific directories\n",
    "HF_PATCHES_DIR = os.path.join(BASE_DATA_DIR, \"patches\")  # Pre-extracted patches from HF\n",
    "HF_TISSUE_SEG_DIR = os.path.join(BASE_DATA_DIR, \"tissue_seg\")  # Pre-computed tissue segmentation\n",
    "HF_ST_DIR = os.path.join(BASE_DATA_DIR, \"st\")  # Spatial transcriptomics data\n",
    "HF_METADATA_DIR = os.path.join(BASE_DATA_DIR, \"metadata\")  # Metadata files\n",
    "\n",
    "# Create output directory if it doesn't exist\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "\n",
    "# Configure pipeline parameters - adapted for HF dataset\n",
    "config = PipelineConfig(\n",
    "    # Segmentation parameters (may use pre-computed from HF)\n",
    "    segmenter='hest',           # Options: 'hest', 'grandqc', or 'precomputed'\n",
    "    seg_conf_thresh=0.5,        # Confidence threshold for segmentation\n",
    "    remove_holes=False,         # Whether to remove holes in tissue\n",
    "    remove_artifacts=False,     # Whether to remove artifacts\n",
    "    \n",
    "    # Patching parameters - HF dataset uses 256x256 patches at 0.5µm/px\n",
    "    mag=40,                     # Magnification corresponding to 0.5µm/px (typically 40x)\n",
    "    patch_size=256,             # Patch size in pixels (HF uses 256x256)\n",
    "    overlap=0,                  # Overlap between patches\n",
    "    min_tissue_proportion=0.0,  # Minimum tissue proportion to keep patch\n",
    "    \n",
    "    # Feature extraction parameters\n",
    "    patch_encoder='conch_v15',  # Available encoders: conch_v15, uni_v1, uni_v2, etc.\n",
    "    mpp=0.5,                    # Microns per pixel (HF dataset uses 0.5µm/px)\n",
    "    \n",
    "    # Processing parameters\n",
    "    batch_size=32,              # Batch size for processing\n",
    "    gpu=0,                      # GPU index to use\n",
    "    skip_errors=False,          # Skip errored slides and continue\n",
    "    reader_type='cucim',        # Reader type for WSI files\n",
    "    \n",
    "    # HF-specific parameters\n",
    "    use_precomputed_patches=True,      # Use pre-extracted patches from HF\n",
    "    use_precomputed_segmentation=True, # Use pre-computed tissue segmentation\n",
    "    hf_patches_dir=HF_PATCHES_DIR,\n",
    "    hf_tissue_seg_dir=HF_TISSUE_SEG_DIR,\n",
    "    hf_st_dir=HF_ST_DIR,\n",
    "    hf_metadata_dir=HF_METADATA_DIR\n",
    ")\n",
    "\n",
    "print(\"Configuration for HF Dataset:\")\n",
    "print(f\"  Base Data Directory: {BASE_DATA_DIR}\")\n",
    "print(f\"  WSI Directory: {WSI_DIR}\")\n",
    "print(f\"  HF Patches Directory: {HF_PATCHES_DIR}\")\n",
    "print(f\"  HF Tissue Segmentation Directory: {HF_TISSUE_SEG_DIR}\")\n",
    "print(f\"  HF Spatial Transcriptomics Directory: {HF_ST_DIR}\")\n",
    "print(f\"  Output Directory: {OUTPUT_DIR}\")\n",
    "print(f\"  Patch Encoder: {config.patch_encoder}\")\n",
    "print(f\"  Magnification: {config.mag}x (0.5µm/px)\")\n",
    "print(f\"  Patch Size: {config.patch_size}px\")\n",
    "print(f\"  Batch Size: {config.batch_size}\")\n",
    "print(f\"  WSI Reader: {config.reader_type}\")\n",
    "print(f\"  MPP: {config.mpp}\")\n",
    "print(f\"  Use Precomputed Patches: {config.use_precomputed_patches}\")\n",
    "print(f\"  Use Precomputed Segmentation: {config.use_precomputed_segmentation}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d883809",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  WSI Reader: image\n"
     ]
    }
   ],
   "source": [
    "# Specify the WSI reader type for HF dataset (Generic TIFF format)\n",
    "# Options: 'openslide', 'image' (for standard TIFFs), 'cucim' (recommended for Generic TIFF)\n",
    "config.reader_type = 'cucim'\n",
    "\n",
    "print(f\"  WSI Reader: {config.reader_type} (optimized for Generic TIFF format)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22d018c9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PROCESSOR] Found 1 valid slides in /home/vcivale/WSI-RL-Tiles-Selection_3/data/interim.\n",
      "Pipeline Manager initialized successfully!\n",
      "Output directory structure created at: /home/vcivale/WSI-RL-Tiles-Selection_3/data/processed\n",
      "  - Segmentation: /home/vcivale/WSI-RL-Tiles-Selection_3/data/processed/segmentation\n",
      "  - Coordinates: /home/vcivale/WSI-RL-Tiles-Selection_3/data/processed/coordinates_20x_512px\n",
      "  - Features: /home/vcivale/WSI-RL-Tiles-Selection_3/data/processed/features_20x_512px_conch_v15\n"
     ]
    }
   ],
   "source": [
    "# Initialize the pipeline manager\n",
    "pipeline_manager = PipelineManager(\n",
    "    job_dir=OUTPUT_DIR,\n",
    "    wsi_dir=WSI_DIR,\n",
    "    config=config,\n",
    "    wsi_ext=WSI_EXTENSIONS,\n",
    "    \n",
    ")\n",
    "\n",
    "print(\"Pipeline Manager initialized successfully!\")\n",
    "print(f\"Output directory structure created at: {OUTPUT_DIR}\")\n",
    "print(f\"  - Segmentation: {pipeline_manager.seg_dir}\")\n",
    "print(f\"  - Coordinates: {pipeline_manager.coords_dir}\")\n",
    "print(f\"  - Features: {pipeline_manager.features_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52c9989e",
   "metadata": {},
   "source": [
    "## 3. Scan Slides and Check Processing Status\n",
    "\n",
    "Discover all WSI files and check the current processing status for each slide."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "08b3a943",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1 WSI samples:\n",
      "  1. TCGA-D3-A2JN-06Z-00-DX1.0AA7684E-2886-4A00-B808-39EA790B825A\n",
      "\n",
      "==================================================\n",
      "PIPELINE SUMMARY\n",
      "==================================================\n",
      "Total samples: 1\n",
      "Segmentation: 0/1 completed\n",
      "Coordinates: 0/1 completed\n",
      "Features: 0/1 completed\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Discover all WSI samples in the directory\n",
    "sample_ids = pipeline_manager.discover_samples()\n",
    "print(f\"Found {len(sample_ids)} WSI samples:\")\n",
    "for i, sample_id in enumerate(sample_ids[:5]):  # Show first 5 samples\n",
    "    print(f\"  {i+1}. {sample_id}\")\n",
    "if len(sample_ids) > 5:\n",
    "    print(f\"  ... and {len(sample_ids) - 5} more\")\n",
    "\n",
    "# Display current processing status\n",
    "pipeline_manager.print_summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "eba60822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Detailed Status:\n",
      "Samples needing segmentation: 1\n",
      "Samples needing coordinates: 0\n",
      "Samples needing features: 0\n",
      "\n",
      "Sample Status Table:\n",
      "                                           Sample ID Segmentation Coordinates  \\\n",
      "0  TCGA-D3-A2JN-06Z-00-DX1.0AA7684E-2886-4A00-B80...            ✗           ✗   \n",
      "\n",
      "  Features Last Updated  \n",
      "0        ✗   2025-07-06  \n"
     ]
    }
   ],
   "source": [
    "# Check which samples need processing for each task\n",
    "seg_pending = pipeline_manager.get_pending_samples(\"segmentation\")\n",
    "coords_pending = pipeline_manager.get_pending_samples(\"coordinates\")\n",
    "feat_pending = pipeline_manager.get_pending_samples(\"features\")\n",
    "\n",
    "print(\"\\nDetailed Status:\")\n",
    "print(f\"Samples needing segmentation: {len(seg_pending)}\")\n",
    "print(f\"Samples needing coordinates: {len(coords_pending)}\")\n",
    "print(f\"Samples needing features: {len(feat_pending)}\")\n",
    "\n",
    "# Create a status DataFrame for better visualization\n",
    "status_data = []\n",
    "for sample_id in sample_ids:\n",
    "    sample_state = pipeline_manager.samples[sample_id]\n",
    "    status_data.append({\n",
    "        'Sample ID': sample_id,\n",
    "        'Segmentation': '✓' if sample_state.segmentation_done else '✗',\n",
    "        'Coordinates': '✓' if sample_state.coordinates_done else '✗',\n",
    "        'Features': '✓' if sample_state.features_done else '✗',\n",
    "        'Last Updated': sample_state.last_updated.split('T')[0] if sample_state.last_updated else 'Never'\n",
    "    })\n",
    "\n",
    "status_df = pd.DataFrame(status_data)\n",
    "print(\"\\nSample Status Table:\")\n",
    "print(status_df.head(10))  # Show first 10 samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35db88b5",
   "metadata": {},
   "source": [
    "## 4. Run Segmentation Where Needed\n",
    "\n",
    "Perform tissue segmentation only on slides that haven't been processed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "74a82233",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logging configured to display detailed process information.\n"
     ]
    }
   ],
   "source": [
    "import logging\n",
    "\n",
    "# Configure logging to capture detailed output from the trident-wsi library\n",
    "# This will help diagnose issues during segmentation or feature extraction\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "print(\"Logging configured to display detailed process information.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c85fea2d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forcing state refresh before segmentation...\n",
      "Force refreshing state by scanning all output files...\n",
      "Checking common Trident output patterns...\n",
      "State refresh completed.\n",
      "\n",
      "==================================================\n",
      "PIPELINE SUMMARY\n",
      "==================================================\n",
      "Total samples: 1\n",
      "Segmentation: 0/1 completed\n",
      "Coordinates: 0/1 completed\n",
      "Features: 0/1 completed\n",
      "==================================================\n",
      "Running segmentation for 1 samples...\n",
      "This may take several minutes depending on the number of slides and their size.\n",
      "Checking for and removing existing segmentation files to ensure a clean run...\n",
      "Running segmentation for 1 samples...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 1 files: 100%|██████████| 1/1 [00:00<00:00, 622.39it/s]\n",
      "Segmenting tissue: 100%|██████████| 1/1 [00:00<00:00, 711.62it/s, TCGA-D3-A2JN-06Z-00-DX1.0AA7684E-2886-4A00-B808-39EA790B825A already segmented. Skipping...]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Segmentation completed for 0 samples.\n",
      "Forcing state refresh after segmentation...\n",
      "Force refreshing state by scanning all output files...\n",
      "Checking common Trident output patterns...\n",
      "State refresh completed.\n",
      "\n",
      "==================================================\n",
      "PIPELINE SUMMARY\n",
      "==================================================\n",
      "Total samples: 1\n",
      "Segmentation: 0/1 completed\n",
      "Coordinates: 0/1 completed\n",
      "Features: 0/1 completed\n",
      "==================================================\n",
      "Warning: 1 samples failed to segment. Check logs for details.\n",
      "\n",
      "==================================================\n",
      "PIPELINE SUMMARY\n",
      "==================================================\n",
      "Total samples: 1\n",
      "Segmentation: 0/1 completed\n",
      "Coordinates: 0/1 completed\n",
      "Features: 0/1 completed\n",
      "==================================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Force a refresh to ensure the state is up-to-date before running\n",
    "print(\"Forcing state refresh before segmentation...\")\n",
    "pipeline_manager.force_refresh_state()\n",
    "seg_pending = pipeline_manager.get_pending_samples(\"segmentation\")\n",
    "\n",
    "if seg_pending:\n",
    "    print(f\"Running segmentation for {len(seg_pending)} samples...\")\n",
    "    print(\"This may take several minutes depending on the number of slides and their size.\")\n",
    "    \n",
    "    # Manually delete existing segmentation files to force re-processing\n",
    "    print(\"Checking for and removing existing segmentation files to ensure a clean run...\")\n",
    "    for sample_id in seg_pending:\n",
    "        seg_file_path = os.path.join(pipeline_manager.seg_dir, f\"{sample_id}.h5\")\n",
    "        if os.path.exists(seg_file_path):\n",
    "            try:\n",
    "                os.remove(seg_file_path)\n",
    "                print(f\"  Removed existing segmentation file: {seg_file_path}\")\n",
    "            except OSError as e:\n",
    "                print(f\"  Error removing file {seg_file_path}: {e}\")\n",
    "\n",
    "    try:\n",
    "        # Run segmentation (without 'force' argument)\n",
    "        processed_count = pipeline_manager.run_segmentation(seg_pending)\n",
    "        print(f\"Segmentation completed for {processed_count} samples.\")\n",
    "        \n",
    "        \n",
    "        # Refresh state immediately after the run to reflect any changes\n",
    "        print(\"Forcing state refresh after segmentation...\")\n",
    "        pipeline_manager.force_refresh_state()\n",
    "\n",
    "        if processed_count < len(seg_pending):\n",
    "            print(f\"Warning: {len(seg_pending) - processed_count} samples failed to segment. Check logs for details.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during segmentation: {e}\")\n",
    "        print(\"Forcing a state refresh to accurately reflect the pipeline status.\")\n",
    "        pipeline_manager.force_refresh_state()\n",
    "    finally:\n",
    "        # Update status\n",
    "        pipeline_manager.print_summary()\n",
    "else:\n",
    "    print(\"All samples already have segmentation completed! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dcbe224",
   "metadata": {},
   "source": [
    "## 5. Run Coordinate Extraction Where Needed\n",
    "\n",
    "Extract patch coordinates for slides that haven't been processed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14dd4e51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples already have coordinates extracted! ✓\n"
     ]
    }
   ],
   "source": [
    "# Run coordinate extraction for samples that need it\n",
    "coords_pending = pipeline_manager.get_pending_samples(\"coordinates\")  # Refresh the list\n",
    "\n",
    "if coords_pending:\n",
    "    print(f\"Running coordinate extraction for {len(coords_pending)} samples...\")\n",
    "    print(\"This step identifies tissue regions and extracts patch coordinates.\")\n",
    "    \n",
    "    # Run coordinate extraction, forcing overwrite\n",
    "    processed_count = pipeline_manager.run_coordinate_extraction(coords_pending, force=True)\n",
    "    print(f\"Coordinate extraction completed for {processed_count} samples.\")\n",
    "    \n",
    "    # Update status\n",
    "    pipeline_manager.print_summary()\n",
    "else:\n",
    "    print(\"All samples already have coordinates extracted! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d0b01a",
   "metadata": {},
   "source": [
    "## 6. Run Patch Feature Extraction Where Needed\n",
    "\n",
    "Extract patch features using the configured patch encoder for slides that haven't been processed yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "54a89385",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All samples already have features extracted! ✓\n"
     ]
    }
   ],
   "source": [
    "# Run feature extraction for samples that need it\n",
    "feat_pending = pipeline_manager.get_pending_samples(\"features\")  # Refresh the list\n",
    "\n",
    "if feat_pending:\n",
    "    print(f\"Running feature extraction for {len(feat_pending)} samples...\")\n",
    "    print(f\"Using patch encoder: {config.patch_encoder}\")\n",
    "    print(\"This is the most computationally intensive step and may take considerable time.\")\n",
    "    \n",
    "    # Run feature extraction, forcing overwrite\n",
    "    processed_count = pipeline_manager.run_feature_extraction(feat_pending, force=True)\n",
    "    print(f\"Feature extraction completed for {processed_count} samples.\")\n",
    "    \n",
    "    # Update status\n",
    "    pipeline_manager.print_summary()\n",
    "else:\n",
    "    print(\"All samples already have features extracted! ✓\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b2b340",
   "metadata": {},
   "source": [
    "## Usage Notes\n",
    "\n",
    "### Key Features:\n",
    "- **Resume Capability**: The pipeline tracks processing state and only runs missing operations\n",
    "- **Organized Output**: Clean directory structure with logical naming conventions\n",
    "- **Error Handling**: Continues processing even if individual slides fail\n",
    "- **Flexible Configuration**: Easy to modify parameters for different use cases\n",
    "\n",
    "### Output Files:\n",
    "- **Segmentation**: `.h5` files containing tissue masks and contours\n",
    "- **Coordinates**: `.h5` files containing patch coordinates and metadata\n",
    "- **Features**: `.h5` files containing patch-level feature embeddings\n",
    "- **State Tracking**: `pipeline_state.json` maintains processing status\n",
    "\n",
    "### Tips:\n",
    "1. **Memory Management**: Adjust `batch_size` based on your GPU memory\n",
    "2. **Storage**: Ensure sufficient disk space for feature files (can be large)\n",
    "3. **Resumption**: You can safely restart the notebook - it will skip completed steps\n",
    "4. **Monitoring**: Check the summary output to track progress\n",
    "\n",
    "### Next Steps:\n",
    "With the extracted features, you can:\n",
    "- Train slide-level classification models\n",
    "- Perform clustering analysis\n",
    "- Generate attention heatmaps\n",
    "- Build retrieval systems"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
